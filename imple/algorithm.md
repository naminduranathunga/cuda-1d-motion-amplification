Algorithm (standard academic pipeline)
1. Video input

Regular RGB or grayscale video

30 FPS is enough

No special camera hardware

2. Spatial decomposition

Apply a Laplacian or Gaussian pyramid per frame.

Purpose:

Separate motion by spatial frequency

Avoid amplifying noise

Mathematically:

Each frame â†’ multi-scale representation

Each scale processed independently

3. Temporal band-pass filtering (key step)

For each pixel at each pyramid level:

Take its intensity over time

Apply a temporal band-pass filter (e.g. IIR / FIR)

This isolates:

Heartbeat range

Small vibrations

Micro facial movements

This step is embarrassingly parallel â†’ perfect for CUDA

4. Motion amplification

Multiply the filtered signal by factor Î±:

ğ¼
â€²
(
ğ‘¥
,
ğ‘¦
,
ğ‘¡
)
=
ğ¼
(
ğ‘¥
,
ğ‘¦
,
ğ‘¡
)
+
ğ›¼
â‹…
ğµ
(
ğ‘¥
,
ğ‘¦
,
ğ‘¡
)
I
â€²
(x,y,t)=I(x,y,t)+Î±â‹…B(x,y,t)

Where:

ğµ
B = band-passed temporal signal

ğ›¼
Î± = amplification factor (e.g. 10â€“50)

5. Reconstruction

Collapse pyramid levels

Rebuild full frame

Write amplified video

Why no high-speed camera is needed
Method	Camera
Optical flow / Lagrangian	Often needs high FPS
Eulerian motion magnification	Normal video works âœ…

Reason:

It amplifies sub-pixel intensity variations, not explicit motion vectors.

Why this is perfect for your GPU CA
GPU-heavy stages

Pyramid construction â†’ convolution

Temporal filtering â†’ per-pixel time series

Reconstruction â†’ convolution

GPU optimization focus

Global vs shared memory

Coalesced frame access

Reusing pyramid buffers

Constant memory for filter coefficients

What you should implement (1-week scope)

Minimal academic version

Grayscale video

Single pyramid level

Simple IIR temporal filter

CUDA kernels:

Frame copy

Temporal filter

Amplification

This is already conference-paper worthy for undergrad.